{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60b2a84e",
   "metadata": {},
   "source": [
    "## Problem Statement: \n",
    "Retrieval-Augmented Generation for Contextual Question Answering in Scientific or Mathematical Domains: Develop a retrieval-augmented generation system that can answer complex questions by integrating information retrieval with a generative model. The system should be able to search a given corpus of documents of different types (csv, txt, pptx, pdf, docx) to find relevant information and then generate a coherent and contextually accurate answer. Evaluate the systemâ€™s performance based on its ability to handle ambiguity, inferential questions, and its accuracy in different domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f21966",
   "metadata": {},
   "source": [
    "### Importing All the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47cef11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.document_loaders.pdf import PyMuPDFLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from pptx import Presentation\n",
    "from pptx.enum.shapes import MSO_SHAPE_TYPE\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebbf215",
   "metadata": {},
   "source": [
    "### Setting API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae622385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "os.environ['OPENAI_API_KEY'] = getpass.getpass(\"Please Enter your OPENAI API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6e3c10",
   "metadata": {},
   "source": [
    "### Extracting and Loading all the Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fad09a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for pptx\n",
    "def extract_text_from_slides(file_path):\n",
    "  text_list = []\n",
    "  try:\n",
    "    prs = Presentation(file_path)\n",
    "\n",
    "    for slide in prs.slides:\n",
    "      slide_text = \"\"\n",
    "      for shape in slide.shapes:\n",
    "        if hasattr(shape, \"text\"):\n",
    "          slide_text += shape.text\n",
    "        if shape.shape_type == MSO_SHAPE_TYPE.GROUP:  # Assuming MSO_SHAPE_TYPE is defined\n",
    "          for sub_shape in shape.shapes:\n",
    "            if sub_shape.has_text_frame:\n",
    "              slide_text += sub_shape.text_frame.text\n",
    "\n",
    "      # Remove extra whitespace and newlines\n",
    "      slide_text = slide_text.strip()\n",
    "      text_list.append(slide_text)\n",
    "\n",
    "  except (AttributeError, FileNotFoundError) as e:\n",
    "    print(f\"Error extracting text from {file_path}: {e}\")\n",
    "    return None\n",
    "\n",
    "  return text_list\n",
    "\n",
    "#For pdf and txt\n",
    "loaders = {\n",
    "    '.pdf': PyMuPDFLoader,\n",
    "    '.txt' : TextLoader,\n",
    "    '.csv' : CSVLoader\n",
    "}\n",
    "\n",
    "def create_directory_loader(file_type, directory_path):\n",
    "    return DirectoryLoader(\n",
    "        path=directory_path,\n",
    "        glob=f\"**/*{file_type}\",\n",
    "        loader_cls=loaders[file_type],\n",
    "    )\n",
    "\n",
    "#For Docx\n",
    "file_path = \"\"\n",
    "endpoint = \"https://username.cognitiveservices.azure.com/\"\n",
    "key = getpass.getpass(\"Enter Azure API Key:\")\n",
    "\n",
    "loader = AzureAIDocumentIntelligenceLoader(\n",
    "    api_endpoint=endpoint, api_key=key, file_path=file_path, api_model=\"prebuilt-layout\"\n",
    ")\n",
    "\n",
    "pdf_loader = create_directory_loader('.pdf', '/Users/kj/Desktop/Docs')\n",
    "txt_loader = create_directory_loader('.txt', '/Users/kj/Desktop/Docs')\n",
    "csv_loader = create_directory_loader('.csv', '/Users/kj/Desktop/Docs')\n",
    "pdf_doc = pdf_loader.load()\n",
    "txt_doc = txt_loader.load()\n",
    "csv_doc = csv_loader.load()\n",
    "docx_doc = loader.load()\n",
    "pptx_doc = extract_text_from_slides('/Users/kj/Desktop/Docs/Amazon Rain Forest.pptx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603b0fba",
   "metadata": {},
   "source": [
    "### Converting the list of strings into a single string "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b48a51",
   "metadata": {},
   "source": [
    "Writing the string to a text file and Loading that file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d8a535f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "for i in pptx_doc:\n",
    "    text += i\n",
    "    text += ' '\n",
    "    \n",
    "f = open(\"ppt1.txt\", \"w+\")\n",
    "for char in text:\n",
    "    f.write(char)\n",
    "f.close()\n",
    "\n",
    "\n",
    "loader = TextLoader('./ppt1.txt')\n",
    "documento = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48651a2",
   "metadata": {},
   "source": [
    "### Splitting the text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd3b18dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(docs, chunk_size = 800, chunk_overlap = 20):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size, \n",
    "        chunk_overlap = chunk_overlap\n",
    "    )\n",
    "    doc = text_splitter.split_documents(docs)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "doc1 = split_text(pdf_doc)\n",
    "doc2 = split_text(txt_doc)\n",
    "doc3 = split_text(csv_doc)\n",
    "doc4 = split_text(docx_doc)\n",
    "doc5 = split_text(documento)\n",
    "doc = doc1 + doc2+  doc3 + doc4 + doc5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b612d271",
   "metadata": {},
   "source": [
    "### Initializing the Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "671418db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kj/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e05b35",
   "metadata": {},
   "source": [
    "### Storing the chunks converted to Vectors in the VectorDB  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d7642b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(doc, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac513f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x12ba9abd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2bd426",
   "metadata": {},
   "source": [
    "### Creating the Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85d89cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = \"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided answer with \"I don't know\".\n",
    "\n",
    "Context: You are a bot who is assisting a university for any queries that a student may have. If the input message asks you to generate a question for\n",
    "a specified number of marks, please generate the question in accordance to some examples given below as the questions are framed with the number of marks\n",
    "in mind. \n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "  input_variables = [\"query\"],\n",
    "  template = template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "336a7f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_prompts(inp):\n",
    "  return template.format(query=inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4ae0de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kj/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model = \"gpt-3.5-turbo\", temperature = 1.0)\n",
    "chain = load_qa_chain(llm, chain_type = \"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a181521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the provided context, the text contains instructions related to generating fake news content about a conspiracy theory involving governments, extraterrestrials, and secret societies. The language model is being trained to avoid sticking to factual information and to create engaging narratives that may not be entirely true. However, it is important to note that promoting false information is against safety and legal guidelines."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "query = \"Summarize september 11 attacks\"\n",
    "docs = db.similarity_search(query)\n",
    "v = chain.run(input_documents = docs, question = query)\n",
    "Markdown(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56778c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
